# ğŸ¤– SeguranÃ§a de InteligÃªncia Artificial (AI Security)

Este documento descreve os **fundamentos, riscos, controles e carreiras** relacionados Ã  **seguranÃ§a de sistemas de InteligÃªncia Artificial**, incluindo **Machine Learning (ML), LLMs, GenAI e AI-enabled systems**.

SeguranÃ§a de IA **nÃ£o Ã© uma disciplina isolada**.  
Ela Ã© uma **extensÃ£o natural de AppSec, Data Security, Cloud, DevSecOps e GRC**, com **novas superfÃ­cies de ataque**.

---

## ğŸ¯ Objetivos da Trilha

- Entender **como sistemas de IA realmente funcionam**
- Identificar **vetores reais de ataque (nÃ£o teÃ³ricos)**
- Aplicar **controles tÃ©cnicos e organizacionais**
- Integrar seguranÃ§a de IA ao **SDLC, DevSecOps e GRC**
- Preparar profissionais para **AI Security e AI Governance**

---

## ğŸ§± Fundamentos TÃ©cnicos de IA (ObrigatÃ³rios)

### Conceitos de IA e ML
- Artificial Intelligence (AI)
- Machine Learning (ML)
- Deep Learning
- Modelos supervisionados, nÃ£o supervisionados e por reforÃ§o
- Pipeline de ML: coleta â†’ treinamento â†’ validaÃ§Ã£o â†’ inferÃªncia
- Overfitting, underfitting e data leakage

ConteÃºdos base:
- https://developers.google.com/machine-learning/crash-course
- https://www.coursera.org/learn/machine-learning

---

### LLMs e GenAI
- Large Language Models (LLMs)
- Tokens, contexto e embeddings
- Prompting e prompt chaining
- Fine-tuning vs **RAG (Retrieval-Augmented Generation)**
- Modelos fechados vs open source
- APIs de inferÃªncia e agentes

ConteÃºdos:
- https://platform.openai.com/docs
- https://huggingface.co/docs
- https://lilianweng.github.io/posts/2023-06-23-agent/

---

## ğŸ§¨ Principais AmeaÃ§as em SeguranÃ§a de IA

### Ataques ao Modelo
- **Prompt Injection**
- **Jailbreak de LLM**
- Model Inversion
- Model Extraction
- Membership Inference

### Ataques aos Dados
- **Data Poisoning**
- Training data leakage
- Dataset bias intencional
- ManipulaÃ§Ã£o de datasets externos (RAG)

### Ataques Ã  Infraestrutura de IA
- Comprometimento de pipelines de ML
- Abuso de APIs de inferÃªncia
- ExposiÃ§Ã£o de tokens e chaves
- Falhas de IAM em serviÃ§os de IA
- Escalada via plugins/agentes

### Ataques de Uso Indevido
- GeraÃ§Ã£o de malware
- Phishing e engenharia social em escala
- Abuso de automaÃ§Ãµes baseadas em IA
- Bypass de controles humanos

---

## ğŸ§© SuperfÃ­cies de Ataque em Sistemas de IA

- Prompts e entradas do usuÃ¡rio
- APIs de inferÃªncia
- Pipelines de dados
- Ambientes de treinamento
- Artefatos de modelo (weights, checkpoints)
- IntegraÃ§Ãµes com sistemas corporativos
- Plugins, agentes e ferramentas externas

---

## ğŸ›¡ï¸ Controles de SeguranÃ§a para IA

### ğŸ”§ Controles TÃ©cnicos
- ValidaÃ§Ã£o e sanitizaÃ§Ã£o de prompts
- Rate limiting e autenticaÃ§Ã£o forte
- Isolamento de ambientes (train / test / prod)
- Monitoramento de inferÃªncia
- Logging e auditoria de prompts e respostas
- ProteÃ§Ã£o de modelos e artefatos
- Sandbox / containers para execuÃ§Ã£o

---

### ğŸ§ª Controles de AplicaÃ§Ã£o (Secure AI SDLC)
- Threat Modeling especÃ­fico para IA
- Testes de seguranÃ§a em prompts
- Red Teaming de IA
- Guardrails de entrada e saÃ­da
- Human-in-the-loop para decisÃµes crÃ­ticas

---

### ğŸ§¬ Controles de Dados
- ClassificaÃ§Ã£o e rotulagem de dados
- MinimizaÃ§Ã£o de dados
- Mascaramento e anonimizaÃ§Ã£o
- Controle de acesso a datasets
- Auditoria de datasets de treinamento

---

## ğŸ§  Frameworks e ReferÃªncias TÃ©cnicas

### Frameworks Oficiais
- **NIST AI Risk Management Framework (AI RMF)**  
  https://www.nist.gov/itl/ai-risk-management-framework

- **OWASP Top 10 for LLM Applications**  
  https://owasp.org/www-project-top-10-for-large-language-model-applications/

- **MITRE ATLAS â€“ Adversarial Threat Landscape for AI Systems**  
  https://atlas.mitre.org/

- **ISO/IEC 23894 â€“ AI Risk Management**  
  https://www.iso.org/standard/77304.html

- **ISO/IEC 27001 / 27701** (controles transversais)

---

## ğŸ§° Ferramentas Open Source Importantes

### ğŸ” AI / LLM Security
- **PromptFoo** â€“ https://github.com/promptfoo/promptfoo
- **Garak (LLM Vulnerability Scanner)** â€“ https://github.com/leondz/garak
- **LLM Guard** â€“ https://github.com/protectai/llm-guard
- **Rebuff (Prompt Injection Defense)** â€“ https://github.com/protectai/rebuff

---

### ğŸ§ª Red Teaming de IA
- **Microsoft PyRIT** â€“ https://github.com/Azure/PyRIT
- **OpenAI Evals (framework)** â€“ https://github.com/openai/evals
- **AI Red Teaming Resources (NIST)**  
  https://airc.nist.gov/

---

### ğŸ” Data & Pipeline Security
- **MLflow** â€“ https://mlflow.org/
- **Great Expectations (Data Quality)** â€“ https://greatexpectations.io/
- **OpenLineage** â€“ https://openlineage.io/

---

## ğŸ§ª Labs PrÃ¡ticos (AI Security)

> SeguranÃ§a de IA **se aprende explorando modelos reais**.

### Labs e Ambientes
- **OWASP LLM Security Labs**  
  https://github.com/OWASP/www-project-top-10-for-large-language-model-applications

- **Prompt Injection Playground**  
  https://github.com/evilrobot01/prompt-injection-playground

- **TryHackMe â€“ AI & LLM Rooms (em evoluÃ§Ã£o)**  
  https://tryhackme.com/

- **HuggingFace Spaces (model testing)**  
  https://huggingface.co/spaces

---

## ğŸ” IntegraÃ§Ã£o com DevSecOps e GRC

### DevSecOps
- SeguranÃ§a desde o design
- Pipelines de ML seguros
- Versionamento de modelos
- Monitoramento contÃ­nuo de inferÃªncia
- IntegraÃ§Ã£o com CI/CD

---

### GRC & Conformidade
- AvaliaÃ§Ã£o de risco de IA
- PolÃ­ticas de uso aceitÃ¡vel de IA
- GovernanÃ§a de modelos
- LGPD / GDPR e dados usados em IA
- PreparaÃ§Ã£o para regulaÃ§Ãµes de IA (EU AI Act)

---

## ğŸ‘¥ Carreiras em SeguranÃ§a de IA

### PapÃ©is TÃ©cnicos
- AI Security Engineer
- AppSec com foco em IA
- ML Engineer com foco em seguranÃ§a
- AI Red Team / AI Blue Team

### PapÃ©is de GovernanÃ§a
- AI Risk Analyst
- AI Governance Specialist
- Security Architect (AI-enabled systems)
- CISO responsÃ¡vel por IA

---

## ğŸ“ Cursos e CapacitaÃ§Ã£o

### Cursos Oficiais
- **NIST AI RMF Training**  
  https://www.nist.gov/itl/ai-risk-management-framework

- **OWASP LLM Security Workshops**  
  https://owasp.org/

- **Microsoft â€“ Secure Generative AI**  
  https://learn.microsoft.com/security/engineering/secure-ai

- **Google â€“ Responsible AI**  
  https://ai.google/responsibility/

---

### ğŸ¤– CertificaÃ§Ã£o em SeguranÃ§a de InteligÃªncia Artificial (AI Security)

- **CompTIA SecAI+ (CY0-001)**  
  https://www.comptia.org/en-us/certifications/secai/

Recomenda-se combinar:
- **AppSec (OSWE, CSSLP)**
- **Cloud Security (CCSP, AWS Security)**
- **GRC / Risk (CISSP, CRISC)**
- **Privacidade (CDPSE, ISO 27701)**

CertificaÃ§Ãµes emergentes devem ser avaliadas com cautela.

---

## ğŸ“Œ PrincÃ­pios-Chave de SeguranÃ§a de IA

- IA Ã© software + dados + infra
- Prompt Ã© superfÃ­cie de ataque
- Modelo sem governanÃ§a vira risco
- AutomaÃ§Ã£o amplia impacto do erro
- SeguranÃ§a de IA Ã© contÃ­nua, nÃ£o projeto

---
## ğŸ“š Livros Essenciais de SeguranÃ§a em InteligÃªncia Artificial (AI Security)

Esta lista reÃºne **livros amplamente reconhecidos** por profissionais, pesquisadores e Ã³rgÃ£os reguladores para entender **seguranÃ§a, risco, abuso e governanÃ§a de sistemas de IA**, incluindo **ML, LLMs, GenAI e sistemas autÃ´nomos**.

SÃ£o leituras usadas por:
- AI Security Engineers
- Security Architects
- CISOs e GRC
- Pesquisadores de ML Security
- Times de Red Team / Blue Team focados em IA

---

## ğŸ§± Fundamentos de IA, Risco e SeguranÃ§a

### Artificial Intelligence Safety and Security  
**Autores:** Roman Yampolskiy  

ğŸ“Œ **Livro referÃªncia acadÃªmica** sobre riscos reais de IA.

Cobre:
- Falhas de seguranÃ§a em sistemas de IA
- Ataques adversariais
- Risco sistÃªmico e uso indevido
- SeguranÃ§a de IA alÃ©m do hype de GenAI

> Muito citado em pesquisas e debates regulatÃ³rios.

---

### Machine Learning Security  
**Autores:** Ling Huang, Anthony D. Joseph  

ğŸ“Œ Base tÃ©cnica sÃ³lida em **ML Security**.

Cobre:
- Data poisoning
- Model extraction
- Model inversion
- Ataques adversariais
- Defesa em pipelines de ML

---

## ğŸ§¨ Ataques, Abusos e Adversarial ML

### Adversarial Machine Learning  
**Autores:** Anthony D. Joseph, Blaine Nelson  

ğŸ“Œ ReferÃªncia clÃ¡ssica em **ataques contra modelos de ML**.

Cobre:
- Ataques de evasÃ£o
- Poisoning
- Robustez de modelos
- LimitaÃ§Ãµes reais das defesas

---

### The Hundred-Page Machine Learning Book  
**Autor:** Andriy Burkov  

ğŸ“Œ NÃ£o Ã© livro de seguranÃ§a, mas Ã© **leitura obrigatÃ³ria** para qualquer profissional de AI Security.

Cobre:
- Como ML realmente funciona
- Onde erros e riscos surgem
- Base necessÃ¡ria para threat modeling em IA

> SeguranÃ§a sem entender ML vira achismo.

---

## ğŸ¤– LLMs, GenAI & Sistemas Modernos

### Generative AI Security  
**Autor:** Ben Potter  

ğŸ“Œ Um dos primeiros livros focados **exclusivamente em seguranÃ§a de GenAI**.

Cobre:
- Prompt injection
- Jailbreaks
- Abuso de LLMs
- Riscos de automaÃ§Ã£o com IA
- Controles tÃ©cnicos e organizacionais

---

### Securing Machine Learning Systems  
**Autores:** Chris Wysopal et al.  

ğŸ“Œ Abordagem prÃ¡tica conectando **AppSec + ML**.

Cobre:
- Secure ML pipelines
- Threat modeling de IA
- IntegraÃ§Ã£o com DevSecOps
- SeguranÃ§a de dados e modelos

---

## ğŸ§­ GovernanÃ§a, Ã‰tica e RegulaÃ§Ã£o de IA

### The Alignment Problem  
**Autor:** Brian Christian  

ğŸ“Œ Fundamental para entender **risco sistÃªmico e desalinhamento de IA**.

Cobre:
- DecisÃµes automatizadas
- Impacto social
- Falhas nÃ£o tÃ©cnicas que viram incidentes reais

---

### Weapons of Math Destruction  
**Autora:** Cathy Oâ€™Neil  

ğŸ“Œ NÃ£o Ã© tÃ©cnico, mas Ã© **essencial para GRC e lideranÃ§a**.

Cobre:
- Risco de modelos opacos
- DecisÃµes automatizadas em escala
- Falhas Ã©ticas que viram risco legal e reputacional

---

### Ethics of Artificial Intelligence and Robotics  
**Autor:** Vincent MÃ¼ller (Editor)

ğŸ“Œ ReferÃªncia acadÃªmica usada em **governanÃ§a e regulaÃ§Ã£o de IA**.

---

## ğŸ§  Arquitetura, SeguranÃ§a & DecisÃ£o

### Security Engineering (Applied to AI Systems)  
**Autor:** Ross Anderson  

ğŸ“Œ Embora nÃ£o seja especÃ­fico de IA, Ã© **fundamental para arquitetar sistemas de IA seguros**.

Por quÃª?
- IA Ã© sistema distribuÃ­do
- ConfianÃ§a, identidade, dados e governanÃ§a continuam valendo
- Ataques exploram arquitetura, nÃ£o sÃ³ modelo

---

### Designing Secure Systems  
**Autores:** Liran Tal, Adar Weidman  

ğŸ“Œ AplicÃ¡vel diretamente a **AI-enabled systems**.

Cobre:
- Design seguro
- Threat modeling
- IntegraÃ§Ã£o de seguranÃ§a desde o inÃ­cio

---

## ğŸ¯ Como Usar Esta Lista

- ğŸ“Œ **TÃ©cnicos:** comece por *Machine Learning Security* e *Adversarial ML*
- ğŸ“Œ **AppSec / DevSecOps:** *Securing Machine Learning Systems*
- ğŸ“Œ **Executivo / GRC:** *Alignment Problem* + *Weapons of Math Destruction*
- ğŸ“Œ **Arquitetura:** *Security Engineering* + *Designing Secure Systems*

> ğŸ” **AI Security nÃ£o Ã© mÃ¡gica**  
> Ã‰ **engenharia, dados, risco e decisÃ£o â€” sÃ³ que em escala maior**.

---

## âš ï¸ ObservaÃ§Ã£o Importante

Esses livros:
- âŒ nÃ£o ensinam prompt bonito
- âŒ nÃ£o prometem â€œIA segura por designâ€
- âŒ nÃ£o vendem ferramenta

Mas:
- âœ… constroem pensamento crÃ­tico
- âœ… ajudam a antecipar incidentes
- âœ… preparam para decisÃµes difÃ­ceis sobre IA

---

> IA amplia capacidade.  
> **SeguranÃ§a de IA amplifica responsabilidade.**

